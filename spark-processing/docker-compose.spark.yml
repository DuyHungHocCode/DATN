version: "3.7"
services:
  spark-master:
    image: bitnami/spark:3.4.0
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_MASTER_HOST=spark-master
    ports:
      - "8090:8080"
      - "7077:7077"
    volumes:
      - ./jobs:/opt/spark/jobs
      - ./lib:/opt/spark/lib
      - ./config/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./config/log4j.properties:/opt/bitnami/spark/conf/log4j.properties
    healthcheck:
      test: curl -f http://localhost:8080 || exit 1
      interval: 30s
      retries: 3
      start_period: 30s
      timeout: 10s
    networks:
      - spark-network
      - kafka-network

  spark-worker-1:
    image: bitnami/spark:3.4.0
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8091:8081"
    volumes:
      - ./jobs:/opt/spark/jobs
      - ./lib:/opt/spark/lib
      - ./config/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./config/log4j.properties:/opt/bitnami/spark/conf/log4j.properties
    depends_on:
      - spark-master
    networks:
      - spark-network

  spark-worker-2:
    image: bitnami/spark:3.4.0
    container_name: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8092:8081"
    volumes:
      - ./jobs:/opt/spark/jobs
      - ./lib:/opt/spark/lib
      - ./config/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./config/log4j.properties:/opt/bitnami/spark/conf/log4j.properties
    depends_on:
      - spark-master
    networks:
      - spark-network

  spark-submit:
    image: bitnami/spark:3.4.0
    container_name: spark-submit
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_URL=spark://spark-master:7077
    volumes:
      - ./jobs:/opt/spark/jobs
      - ./lib:/opt/spark/lib
      - ./scripts:/opt/spark/scripts
      - ./config/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./config/log4j.properties:/opt/bitnami/spark/conf/log4j.properties
    command: 
      - /bin/bash
      - -c
      - |
        echo "Waiting for Spark master to be ready..."
        sleep 30
        echo "Installing required Python packages..."
        pip install pyspark==3.4.0 pymongo==4.4.0 pandas==2.0.0 numpy==1.24.0 kafka-python==2.0.2 confluent-kafka==2.1.1 avro-python3==1.10.2 fastavro==1.7.3
        echo "Setting up Spark environment..."
        mkdir -p /opt/bitnami/spark/python/lib
        cp -r /usr/local/lib/python3.*/site-packages/* /opt/bitnami/spark/python/lib/
        echo "Running job submission script..."
        chmod +x /opt/spark/scripts/submit-job.sh
        /opt/spark/scripts/submit-job.sh
    depends_on:
      - spark-master
      - spark-worker-1
      - spark-worker-2
    networks:
      - spark-network
      - kafka-network
      - mongodb-network

networks:
  spark-network:
    driver: bridge
  kafka-network:
    external: true
  mongodb-network:
    external: true